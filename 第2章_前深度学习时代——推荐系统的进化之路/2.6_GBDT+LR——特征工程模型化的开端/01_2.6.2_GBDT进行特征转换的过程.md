# 01_2.6.2 GBDT进行特征转换的过程

"""
Lecture: 第2章 前深度学习时代——推荐系统的进化之路/2.6 GBDT+LR——特征工程模型化的开端
Content: 01_2.6.2 GBDT进行特征转换的过程
"""

### 2.6.2 GBDT进行特征转换的过程

#### 背景介绍

GBDT（梯度提升决策树）是一种强大的集成学习算法，通过集成多个决策树来提高模型的预测精度。在GBDT+LR组合模型中，GBDT不仅用于模型训练，还用于特征工程，通过自动生成新的特征向量来增强LR模型的输入。

#### 特征转换过程

GBDT进行特征转换的具体过程如下：

1. **训练GBDT模型**：
   - 利用训练集训练GBDT模型，每一棵子树都是通过拟合前一轮的残差来生成的。GBDT模型由多棵回归树组成，每棵树的生成过程都是标准的回归树生成过程。

2. **生成离散特征向量**：
   - 训练好GBDT模型后，可以利用该模型将原始特征向量转换为新的离散特征向量。具体过程如下：
     - 一个训练样本在输入GBDT的某一子树后，根据每个节点的规则最终落入某一个叶子节点。将该叶子节点置为1，其他叶子节点置为0，所有叶子节点组成的向量即形成了该棵树的特征向量。
     - 将GBDT所有子树的特征向量连接起来，即形成了后续LR模型输入的离散型特征向量。

#### 具体例子

举例来说，如图2-17所示，GBDT由三棵子树构成，每棵子树有4个叶子节点：

- 输入一个训练样本后，其先后落入“子树1”的第3个叶节点中，那么特征向量就是[0, 0, 1, 0]。
- “子树2”的第1个叶节点，特征向量为[1, 0, 0, 0]。
- “子树3”的第4个叶节点，特征向量为[0, 0, 0, 1]。
- 最后连接所有特征向量，形成最终的特征向量[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]。

#### 决策树深度与特征交叉

决策树的深度决定了特征交叉的阶数。如果决策树的深度为4，则通过3次节点分裂，最终的叶节点实际上是进行三阶特征组合后的结果。这种强大的特征组合能力使得GBDT在特征工程中表现出色。然而，GBDT的特征转换方式也有一些限制，例如丢失了大量特征的数值信息，这在某些情况下可能会影响模型的表现。

#### 优缺点分析

1. **优点**：
   - **自动化特征工程**：GBDT通过自动选择和组合特征，有效地减轻了人工特征工程的负担。
   - **高效特征组合**：GBDT能够高效地进行高阶特征组合，这种能力是很多其他模型所不具备的。
   - **灵活性强**：GBDT可以与其他模型（如LR）结合使用，增强模型的整体表现。

2. **缺点**：
   - **过拟合风险**：GBDT容易产生过拟合，尤其是在树的数量过多或者树的深度过大时。
   - **特征信息丢失**：GBDT的特征转换方式可能会丢失一些数值特征的信息，这在某些情况下可能会影响模型的性能。
   - **计算复杂度高**：GBDT的训练和特征转换过程都需要大量的计算资源，对于大规模数据集来说，计算复杂度较高。

#### 总结

GBDT在特征工程中的应用大大提高了特征选择和组合的效率，通过自动生成新的离散特征向量，为后续的LR模型提供了更为丰富的输入特征。这种组合模型在实际应用中表现出色，尽管存在一些挑战，但通过合理的参数调节和优化策略，GBDT+LR模型在推荐系统中得到了广泛应用。

---

### 决策树深度与特征交叉的关系

#### 决策树的基本结构

决策树是一种树状结构的模型，通过一系列的决策规则将数据划分为不同的子集。每个节点表示一个特征，每条分支代表该特征的一个取值或区间，叶节点表示最终的预测结果或类别。决策树的深度决定了从根节点到叶节点的最长路径，也即决策树中分裂次数最多的路径。

#### 特征交叉的概念

特征交叉（Feature Interaction）是指组合多个特征来捕捉它们之间的相互作用。例如，对于特征 $ x_1 $、$ x_2 $ 和 $ x_3 $，它们的三阶交叉特征可以表示为 $ x_1 \cdot x_2 \cdot x_3 $。特征交叉可以捕捉到单个特征无法表达的信息，提高模型的预测能力。

#### 决策树深度与特征交叉

当决策树的深度为4时，意味着从根节点到叶节点最多经过4层，每层对应一次特征分裂。每次分裂选择一个特征，并将数据集按该特征的取值或区间划分为不同的子集。这样，通过4次分裂，可以捕捉到最多4个特征的相互作用。

具体分析如下：

1. **根节点分裂**：
   - 选择特征 $ x_1 $ 进行分裂，将数据集按 $ x_1 $ 的取值或区间划分为两个子集。

2. **第一层节点分裂**：
   - 对于每个子集，选择特征 $ x_2 $ 进行第二次分裂，将每个子集再次划分为两个子集。

3. **第二层节点分裂**：
   - 对于每个新的子集，选择特征 $ x_3 $ 进行第三次分裂，将每个子集继续划分为两个子集。

4. **第三层节点分裂**：
   - 对于每个进一步划分的子集，选择特征 $ x_4 $ 进行第四次分裂，将每个子集最终划分为两个叶节点。

在这种情况下，最终的叶节点是通过特征 $ x_1 $、$ x_2 $、$ x_3 $ 和 $ x_4 $ 的分裂路径得到的。也就是说，一个样本要落入某个特定的叶节点，需要满足所有这4个特征的分裂条件。这相当于捕捉了这4个特征的交互作用。

#### 三阶特征组合的解释

尽管决策树的深度为4，但最终的叶节点实际上是通过3次节点分裂得到的。这是因为：

- 第一次分裂选择了第一个特征（例如 $ x_1 $），将数据集划分为两个子集。
- 第二次分裂选择了第二个特征（例如 $ x_2 \）），将每个子集再次划分为两个子集。
- 第三次分裂选择了第三个特征（例如 $ x_3 $），将每个子集继续划分为两个子集。

通过3次分裂，数据集已经被划分到了叶节点。因此，每个叶节点实际上是由3个特征的交互作用决定的，即每个叶节点对应于一个三阶特征组合。这种特征组合可以捕捉到比单一特征更复杂的相互作用信息，提高模型的预测能力。

#### 总结

决策树的深度决定了特征交叉的阶数。当决策树的深度为4时，通过3次节点分裂，最终的叶节点实际上是进行三阶特征组合后的结果。这种特征交叉方式可以有效地捕捉多个特征之间的相互作用，提高模型的表现和预测能力。通过GBDT进行特征转换，可以自动生成高阶特征组合，为后续的模型提供丰富的输入特征。