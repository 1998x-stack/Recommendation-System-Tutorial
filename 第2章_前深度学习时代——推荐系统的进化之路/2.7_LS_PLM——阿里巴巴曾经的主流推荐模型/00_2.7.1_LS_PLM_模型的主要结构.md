# 00_2.7.1 LS-PLM 模型的主要结构

"""
Lecture: 第2章 前深度学习时代——推荐系统的进化之路/2.7 LS-PLM——阿里巴巴曾经的主流推荐模型
Content: 00_2.7.1 LS-PLM 模型的主要结构
"""

### 2.7.1 LS-PLM 模型的主要结构

#### 背景介绍

LS-PLM（Large Scale Piece-wise Linear Model），又称为混合逻辑回归（Mixed Logistic Regression，MLR）模型，是阿里巴巴曾经的主流推荐模型。该模型早在2012年就已经应用于阿里巴巴的广告推荐场景，并在2017年被正式公布。LS-PLM的出现连接了传统推荐模型和深度学习推荐模型两个时代，是特征工程自动化和模型端到端训练的重要尝试。

#### 模型结构概述

LS-PLM的结构与三层神经网络极其相似，主要由以下几个部分组成：

1. **输入层**：
   - 输入层是样本的特征向量，包括用户特征、物品特征以及上下文特征。

2. **中间层（隐层）**：
   - 中间层是由多个神经元（即分片）组成的隐层，每个分片对应一个逻辑回归模型。LS-PLM通过分而治之的思路，先对样本进行分片，然后在每个分片中应用逻辑回归模型进行点击率（CTR）预估。

3. **输出层**：
   - 输出层是由单一神经元组成的输出层，用于生成最终的CTR预测结果。LS-PLM通过将每个分片的预测结果加权求和，得到最终的预测值。

#### 数学形式

LS-PLM的数学形式如下：

1. **样本分片**：
   $$
   \pi = \text{softmax}(w_{\pi} \cdot x)
   $$
   其中，$ \pi $ 表示样本分片的概率，$ w_{\pi} $ 是分片的权重，$ x $ 是样本的特征向量，softmax函数用于对样本进行多分类。

2. **逻辑回归**：
   $$
   \hat{y}_i = \sigma(w_i \cdot x)
   $$
   其中，$ \hat{y}_i $ 是分片 $ i $ 的预测值，$ w_i $ 是逻辑回归模型的权重，$ x $ 是样本的特征向量，$ \sigma $ 是sigmoid函数。

3. **最终预测**：
   $$
   \hat{y} = \sum_{i} \pi_i \cdot \hat{y}_i
   $$
   其中，$ \hat{y} $ 是最终的CTR预测结果，$ \pi_i $ 是样本属于分片 $ i $ 的概率，$ \hat{y}_i $ 是分片 $ i $ 的预测值。

#### 优缺点分析

1. **优点**：
   - **非线性学习能力**：LS-PLM具有样本分片的能力，可以挖掘出数据中蕴藏的非线性模式，省去了大量的人工样本处理和特征工程过程，使得模型可以端到端地完成训练。
   - **模型的稀疏性**：LS-PLM在建模时引入了L1和L2正则化，使最终训练出来的模型具有较高的稀疏度，模型部署更加轻量级，在线推断效率更高。

2. **缺点**：
   - **计算复杂度高**：由于LS-PLM需要对样本进行分片和分别训练多个逻辑回归模型，计算复杂度较高，特别是在大规模数据集上训练时，需要大量的计算资源。
   - **模型训练难度大**：LS-PLM的训练过程涉及多个逻辑回归模型的训练和分片策略的优化，需要对模型进行细致的调优和参数选择。

#### 应用与优化

1. **应用场景**：
   - LS-PLM适用于工业级的推荐系统、广告系统等大规模稀疏数据场景，特别是在需要进行点击率预估和用户行为预测的场景中，具有广泛的应用前景。

2. **优化策略**：
   - **超参数调节**：在实践中，可以通过调节分片数 $ m $ 来平衡模型的拟合能力与推广能力。经验值表明， $ m $ 取12时，模型表现较优。
   - **正则化**：通过引入L1和L2正则化项，可以提高模型的稀疏性，减少过拟合风险，提升模型的泛化能力。

#### 总结

LS-PLM模型通过将逻辑回归与样本分片相结合，实现了非线性特征学习和特征工程自动化，是连接传统推荐模型和深度学习推荐模型的重要节点。尽管存在一定的计算复杂度和训练难度，但其在实际应用中表现出了强大的特征学习和预测能力，为推荐系统的发展做出了重要贡献。