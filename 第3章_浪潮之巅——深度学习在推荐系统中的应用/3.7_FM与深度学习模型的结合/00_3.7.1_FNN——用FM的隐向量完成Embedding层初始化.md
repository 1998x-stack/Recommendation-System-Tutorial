# 00_3.7.1 FNN——用FM的隐向量完成Embedding层初始化

"""
Lecture: 第3章 浪潮之巅——深度学习在推荐系统中的应用/3.7 FM与深度学习模型的结合
Content: 00_3.7.1 FNN——用FM的隐向量完成Embedding层初始化
"""

### 3.7.1 FNN——用FM的隐向量完成Embedding层初始化

#### 1. 背景和动机
在深度学习的推荐系统中，Embedding层的初始化对模型的训练速度和效果有着重要影响。传统的随机初始化方法虽然简单，但往往不包含任何先验信息，导致Embedding层的收敛速度较慢，影响整个神经网络的训练效率。为了提升Embedding层的初始化质量，FNN（Factorization Machine supported Neural Network）模型应运而生。

#### 2. FNN模型概述
FNN模型由伦敦大学学院的研究人员于2016年提出，模型结构如图3-17所示。初看之下，FNN模型类似于经典的深度神经网络模型，例如Deep Crossing，从稀疏输入向量到稠密向量的转换过程也是经典的Embedding层结构。那么，FNN模型与FM模型是如何结合的呢？关键在于Embedding层的初始化方式。

#### 3. Embedding层的初始化改进
在FNN模型中，Embedding层的初始化不再采用传统的随机方法，而是使用FM（Factorization Machine）模型训练得到的隐向量。FM模型通过对特征进行隐向量分解，能够捕捉到特征之间的交互信息，这些隐向量包含了有价值的先验知识。将这些隐向量作为Embedding层的初始权重，相当于在神经网络训练的起点上已经接近目标最优点，从而加速整个模型的收敛过程。

#### 4. 为什么Embedding层的收敛速度慢
在深度学习网络中，Embedding层的作用是将稀疏输入向量转换成稠密向量。然而，由于以下原因，Embedding层的收敛速度往往较慢：
1. **参数数量巨大**：假设输入层的维度是100,000，Embedding层的输出维度是32，上层再加5层32维的全连接层，最终输出层维度是10，那么Embedding层的参数数量是32×100,000=3,200,000，而其余所有层的参数总数是4416。Embedding层的参数占比达到99.86%。
2. **稀疏输入的影响**：在随机梯度下降的过程中，只有与非零特征相连的Embedding层权重会被更新，进一步降低了Embedding层的收敛速度。

#### 5. FNN模型的具体实现
在FNN模型中，FM模型的隐向量用于初始化Embedding层的参数。具体过程如下：
1. **FM模型训练**：首先使用FM模型对特征进行训练，得到各特征的隐向量。
2. **初始化Embedding层**：将这些隐向量作为Embedding层的初始权重。假设FM隐向量的维度为m，第i个特征域的第k维特征的隐向量为$$v_{ik}$$，那么隐向量的第l维$$v_{il}$$就会成为连接输入神经元k和Embedding神经元l之间连接权重的初始值。
3. **多特征域的处理**：在FNN模型中，不同特征被分成不同特征域，每个特征域具有对应的Embedding层，且每个特征域的Embedding维度应与FM隐向量维度保持一致。

#### 6. FNN模型的优势
使用FM模型的隐向量初始化Embedding层权重，FNN模型在以下方面具有显著优势：
1. **收敛速度加快**：由于初始化时已经引入了有价值的先验信息，整个神经网络的训练过程更加高效。
2. **参数优化**：Embedding层的初始权重更接近最终的最优值，减少了训练过程中参数调整的幅度。

#### 7. 总结
FNN模型通过将FM模型的隐向量用于Embedding层的初始化，克服了传统随机初始化方法的不足，显著提升了Embedding层的收敛速度和模型的训练效率。这种创新性的结合方式，不仅在理论上提供了新的思路，也在实际应用中展现了强大的性能优势。