
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>00_4.2.1_什么是Word2vec</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">
</head>
<body>
  <div class="container">
    <h1>00_4.2.1 什么是Word2vec</h1>
<p>&quot;&quot;&quot;
Lecture: 第4章 Embedding技术在推荐系统中的应用/4.2 Word2vec——经典的Embedding方法
Content: 00_4.2.1 什么是Word2vec
&quot;&quot;&quot;</p>
<h3>4.2.1 什么是Word2vec</h3>
<h4>一、基本概念和背景</h4>
<p>Word2vec是由Google于2013年提出的用于生成词向量的模型。它的核心思想是将词语映射到一个低维稠密向量空间，使得语义相近的词在向量空间中距离较近，而语义不相关的词距离较远。Word2vec模型的提出标志着自然语言处理（NLP）领域的一个重要里程碑，并为后续的Embedding技术奠定了基础。</p>
<h4>二、Word2vec的模型结构</h4>
<p>Word2vec模型有两种主要的结构：连续词袋模型（CBOW）和跳跃模型（Skip-Gram）。</p>
<ol>
<li>
<p><strong>CBOW模型</strong>：</p>
<ul>
<li>目标：通过上下文词预测中心词。</li>
<li>示例：在句子&quot;The cat sits on the mat&quot;中，通过上下文词[&quot;The&quot;, &quot;cat&quot;, &quot;on&quot;, &quot;the&quot;]预测中心词&quot;sits&quot;。</li>
</ul>
</li>
<li>
<p><strong>Skip-Gram模型</strong>：</p>
<ul>
<li>目标：通过中心词预测上下文词。</li>
<li>示例：在句子&quot;The cat sits on the mat&quot;中，通过中心词&quot;sits&quot;预测上下文词[&quot;The&quot;, &quot;cat&quot;, &quot;on&quot;, &quot;the&quot;]。</li>
</ul>
</li>
</ol>
<p>经验上讲，Skip-Gram模型在处理大规模语料时效果较好，因此在实际应用中较为常用。</p>
<h4>三、Word2vec的训练过程</h4>
<ol>
<li>
<p><strong>构建训练样本</strong>：</p>
<ul>
<li>通过滑动窗口从语料库中抽取训练样本。假设窗口大小为2，则句子&quot;The cat sits on the mat&quot;会生成以下训练样本：[(&quot;The&quot;, &quot;cat&quot;), (&quot;cat&quot;, &quot;sits&quot;), (&quot;sits&quot;, &quot;on&quot;), (&quot;on&quot;, &quot;the&quot;), (&quot;the&quot;, &quot;mat&quot;)]。</li>
</ul>
</li>
<li>
<p><strong>定义优化目标</strong>：</p>
<ul>
<li>采用极大似然估计的方法，目标是最大化所有训练样本的条件概率之积。</li>
<li>优化目标公式为：![](https://latex.codecogs.com/png.latex?\sum_{t=1}^{T}\sum_{-c\leq j\leq c, j\neq 0}\log{p(w_{t+j}|w_t)})。</li>
</ul>
</li>
<li>
<p><strong>计算条件概率</strong>：</p>
<ul>
<li>使用softmax函数计算条件概率。</li>
<li>条件概率公式为：![](https://latex.codecogs.com/png.latex?p(w_O|w_I)=\frac{exp(v_{w_O}^T\cdot v_{w_I})}{\sum_{w=1}^{W}exp(v_w^T\cdot v_{w_I})})，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mrow><msub><mi>w</mi><mi>O</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">v_{w_O}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.68556em;vertical-align:-0.255em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mrow><msub><mi>w</mi><mi>I</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">v_{w_I}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.68556em;vertical-align:-0.255em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>分别为输出词和输入词的向量表示。</li>
</ul>
</li>
</ol>
<h4>四、Word2vec的实现细节</h4>
<ol>
<li>
<p><strong>负采样（Negative Sampling）</strong>：</p>
<ul>
<li>目的是简化softmax计算的复杂度。</li>
<li>通过只计算采样出的负样本的预测误差，减少计算量。</li>
<li>优化目标公式为：![](https://latex.codecogs.com/png.latex?L=\log{\sigma(v_{w_O}^T\cdot h)}+\sum_{i=1}^{k}E_{w_i\sim P_n(w)}[\log{\sigma(-v_{w_i}^T\cdot h)}])，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">h</span></span></span></span>为隐层向量，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mrow><msub><mi>w</mi><mi>O</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">v_{w_O}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.68556em;vertical-align:-0.255em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">O</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>为输出词向量，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>为负样本。</li>
</ul>
</li>
<li>
<p><strong>层级softmax（Hierarchical Softmax）</strong>：</p>
<ul>
<li>通过构建霍夫曼树加快softmax计算。</li>
<li>在每次预测时，只需计算从根节点到目标词的路径上的节点概率。</li>
</ul>
</li>
</ol>
<h4>五、Word2vec的应用</h4>
<ol>
<li>
<p><strong>文本分类</strong>：</p>
<ul>
<li>通过将文本转换为词向量，输入到分类模型中进行文本分类。</li>
<li>应用示例：垃圾邮件分类、新闻分类。</li>
</ul>
</li>
<li>
<p><strong>情感分析</strong>：</p>
<ul>
<li>通过词向量表示的文本数据，可以更好地捕捉文本中的情感信息。</li>
<li>应用示例：电影评论情感分析、社交媒体情感分析。</li>
</ul>
</li>
<li>
<p><strong>机器翻译</strong>：</p>
<ul>
<li>通过将源语言和目标语言的词向量映射到相同的向量空间，实现跨语言的文本转换。</li>
<li>应用示例：Google翻译、Bing翻译。</li>
</ul>
</li>
</ol>
<h4>六、Word2vec的优势和局限性</h4>
<ol>
<li>
<p><strong>优势</strong>：</p>
<ul>
<li>能够捕捉词语的语义关系。</li>
<li>计算效率高，适用于大规模语料。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li>无法处理多义词的不同语义。</li>
<li>对于长距离依赖关系的捕捉能力有限。</li>
</ul>
</li>
</ol>
<h4>七、Word2vec对Embedding技术的影响</h4>
<p>Word2vec的提出不仅在自然语言处理领域引起了广泛关注，还推动了Embedding技术在其他领域的应用。通过Word2vec，Embedding技术在广告、搜索、推荐系统等领域得到了广泛应用，成为深度学习知识框架中不可或缺的一部分。</p>
<h3>总结</h3>
<p>Word2vec作为经典的Embedding方法，通过将高维稀疏的文本数据转换为低维稠密的向量，使得计算机能够更高效地处理和理解文本。其模型结构、训练方法和优化目标对后续的Embedding研究具有重要的启发意义。掌握Word2vec的每一个细节，对于理解和应用Embedding技术至关重要。</p>

  </div>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false},
          {left: "\(", right: "\)", display: false},
          {left: "\[", right: "\]", display: true}
        ]
      });
    });
  </script>
</body>
</html>
  