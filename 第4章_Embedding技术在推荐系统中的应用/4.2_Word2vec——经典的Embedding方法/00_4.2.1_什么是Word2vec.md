# 00_4.2.1 什么是Word2vec

"""
Lecture: 第4章 Embedding技术在推荐系统中的应用/4.2 Word2vec——经典的Embedding方法
Content: 00_4.2.1 什么是Word2vec
"""

### 4.2.1 什么是Word2vec

#### 一、基本概念和背景

Word2vec是由Google于2013年提出的用于生成词向量的模型。它的核心思想是将词语映射到一个低维稠密向量空间，使得语义相近的词在向量空间中距离较近，而语义不相关的词距离较远。Word2vec模型的提出标志着自然语言处理（NLP）领域的一个重要里程碑，并为后续的Embedding技术奠定了基础。

#### 二、Word2vec的模型结构

Word2vec模型有两种主要的结构：连续词袋模型（CBOW）和跳跃模型（Skip-Gram）。

1. **CBOW模型**：
   - 目标：通过上下文词预测中心词。
   - 示例：在句子"The cat sits on the mat"中，通过上下文词["The", "cat", "on", "the"]预测中心词"sits"。

2. **Skip-Gram模型**：
   - 目标：通过中心词预测上下文词。
   - 示例：在句子"The cat sits on the mat"中，通过中心词"sits"预测上下文词["The", "cat", "on", "the"]。

经验上讲，Skip-Gram模型在处理大规模语料时效果较好，因此在实际应用中较为常用。

#### 三、Word2vec的训练过程

1. **构建训练样本**：
   - 通过滑动窗口从语料库中抽取训练样本。假设窗口大小为2，则句子"The cat sits on the mat"会生成以下训练样本：[("The", "cat"), ("cat", "sits"), ("sits", "on"), ("on", "the"), ("the", "mat")]。

2. **定义优化目标**：
   - 采用极大似然估计的方法，目标是最大化所有训练样本的条件概率之积。
   - 优化目标公式为：![](https://latex.codecogs.com/png.latex?\sum_{t=1}^{T}\sum_{-c\leq j\leq c, j\neq 0}\log{p(w_{t+j}|w_t)})。

3. **计算条件概率**：
   - 使用softmax函数计算条件概率。
   - 条件概率公式为：![](https://latex.codecogs.com/png.latex?p(w_O|w_I)=\frac{exp(v_{w_O}^T\cdot v_{w_I})}{\sum_{w=1}^{W}exp(v_w^T\cdot v_{w_I})})，其中$v_{w_O}$和$v_{w_I}$分别为输出词和输入词的向量表示。

#### 四、Word2vec的实现细节

1. **负采样（Negative Sampling）**：
   - 目的是简化softmax计算的复杂度。
   - 通过只计算采样出的负样本的预测误差，减少计算量。
   - 优化目标公式为：![](https://latex.codecogs.com/png.latex?L=\log{\sigma(v_{w_O}^T\cdot h)}+\sum_{i=1}^{k}E_{w_i\sim P_n(w)}[\log{\sigma(-v_{w_i}^T\cdot h)}])，其中$h$为隐层向量，$v_{w_O}$为输出词向量，$w_i$为负样本。

2. **层级softmax（Hierarchical Softmax）**：
   - 通过构建霍夫曼树加快softmax计算。
   - 在每次预测时，只需计算从根节点到目标词的路径上的节点概率。

#### 五、Word2vec的应用

1. **文本分类**：
   - 通过将文本转换为词向量，输入到分类模型中进行文本分类。
   - 应用示例：垃圾邮件分类、新闻分类。

2. **情感分析**：
   - 通过词向量表示的文本数据，可以更好地捕捉文本中的情感信息。
   - 应用示例：电影评论情感分析、社交媒体情感分析。

3. **机器翻译**：
   - 通过将源语言和目标语言的词向量映射到相同的向量空间，实现跨语言的文本转换。
   - 应用示例：Google翻译、Bing翻译。

#### 六、Word2vec的优势和局限性

1. **优势**：
   - 能够捕捉词语的语义关系。
   - 计算效率高，适用于大规模语料。

2. **局限性**：
   - 无法处理多义词的不同语义。
   - 对于长距离依赖关系的捕捉能力有限。

#### 七、Word2vec对Embedding技术的影响

Word2vec的提出不仅在自然语言处理领域引起了广泛关注，还推动了Embedding技术在其他领域的应用。通过Word2vec，Embedding技术在广告、搜索、推荐系统等领域得到了广泛应用，成为深度学习知识框架中不可或缺的一部分。

### 总结

Word2vec作为经典的Embedding方法，通过将高维稀疏的文本数据转换为低维稠密的向量，使得计算机能够更高效地处理和理解文本。其模型结构、训练方法和优化目标对后续的Embedding研究具有重要的启发意义。掌握Word2vec的每一个细节，对于理解和应用Embedding技术至关重要。